## Stable Diffusion - Image to Prompts
Deduce the prompts that generated our "highly detailed, sharp focus, illustration, 3d renders of majestic, epic" images

This repository contains:
## Table of Contents
- [Background](#background)
- [Dataset](#dataset)
- [Usage](#usage)
- [Example](#example)
- [Related Efforts](#related-efforts)
- [Maintainers](#maintainers)
- [Contributing](#contributing)
- [License](#license)
## Background
The goal of this Topic is to reverse the typical direction of a generative text-to-image model: instead of generating an image from a text prompt, we will create a model which can predict the text prompt given a generated image, we will make predictions on a dataset containing a wide variety of (prompt, image) pairs generated by Stable Diffusion 2.0, in order to understand how reversible the latent relationship is.
## Dataset
from datasets import load_dataset


subset = '2m_first_1k' or '2m_first_5k'


dataset = load_dataset("poloclub/diffusiondb", subset)


For more dataset, please visit https://huggingface.co/datasets/poloclub/diffusiondb
## Usage
For CLIP+GPT2 Model,


Click into Final/CLIP_GPT2 and use this directory as the root directory, run train_clip.py for training, predict.py for precition.


For ResNet+LSTM Model,


Click into Final/Resnet_LSTM_GPT2 and use this directory as the root directory, run model.py for training, predict.py for precition.


If you want to change the dataset or parameters, change the corresponding code in train_clip.py and model.py before running.

## Example





# COMP9417_Project
No. | Deep learning | Name
| :----- | :----- | :----- 
1 | ResNet+LSTM | Jinghan Wang
2 | CLIP+GPT2 | Shengyu Chen
